{
  "hash": "dfa8fe9f440803005335126845f4bf4a",
  "result": {
    "markdown": "---\ntitle: \"The Beginner's Blueprint: Crafting a Documented Path in Your R Data Analysis Journey\"\nauthor: \"Soundarya Soundararajan\"\ndate: \"2024-01-09\"\ncategories: [documentation, analysis, beginners]\nimage: \"files.jpg\"\neditor_options: \n  chunk_output_type: console\nbibliography: references.bib\n---\n\n\n> **This post offers a disciplined approach to maintaining research logs and a proactive mindset for efficient and standardized data analysis sessions.**\n\n![Photo by Beatriz Pérez Moya on Unsplash](files.jpg){fig-align=\"center\"}\n\nWhen I was asked for a detailed analysis conducted last month, I faced two challenges. The steps were really detailed, making a lot of decisions necessary, and remembering the exact process a month later was tough. So, I turned to my log files.\n\nI keep detailed logs of every step in my analysis. While my main results file has the big decisions, the log files are like prompts, capturing every little thing with timestamps. Even though I could remember the main process, the detailed parts were much clearer in my log files than in my memory.\n\nMaintaining these logs with the `logr` package [@bosak2022] turned out to be a smart move. Writing things down as I go makes it much easier than trying to recall everything later. This idea aligns with Long's law of documentation:\n\n> **\"It's always faster to document it today than tomorrow.\"**\n\nI learned this from J. Scott Long's Stata book[@long2008workflowdataanalysis], a great resource, especially for those learning to code. Following this principle, I only wrap up an analysis session when I'm confident that the key details are safely stored in my log files.\n\nThis is a typical analysis session simplified.\n\n## Packages\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| message: false\n#| warning: false\npacman::p_load(\"logr\",\"here\",\"palmerpenguins\",\"tidyverse\")\n```\n````\n:::\n\n\n## Counting the species\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| message: false\n#| warning: false\npenguins |> \n        count(species)\n```\n````\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n```\n:::\n:::\n\n\n## Dataviz\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| message: false\n#| warning: false\npenguins |> \n        ggplot(aes(species,bill_length_mm,fill=species))+\n        geom_boxplot()\n```\n````\n\n::: {.cell-output-display}\n![](logr_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n### With different colors\n\n\n::: {.cell}\n\n````{.cell-code}\n```{{r}}\n#| message: false\n#| warning: false\npenguins |> \n        ggplot(aes(species,bill_length_mm,fill=species))+\n        geom_boxplot()+\n        scale_fill_manual(values = c(\n                \"midnightblue\", \"orange\", \"seagreen\"\n        ))+\n        theme_minimal()\n```\n````\n\n::: {.cell-output-display}\n![](logr_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nAs I tackle this, I open an R script file and immediately begin jotting down my logs. Check out the script of my log file capturing the entire analysis session below.\n\n![Script of my log file](logscript.png){fig-align=\"center\"}\n\nEngaging in this process, I initiate an R script file and promptly commence logging each step. Witness the script of my log file documenting the entire analysis session below. Running this script in parallel with my analysis generates and populates a log output file. I've included screenshots of it for your reference below.\n\n![](logoutpu1.png){fig-align=\"left\" width=\"600\"}\n\n![](logoutput2.png){fig-align=\"right\" width=\"600\"}\n\n## How I learnt to do it better\n\n1.  Reproducible Logging Script:\n\nI've established a fundamental script that serves as my go-to template. Every time I embark on an R project for analysis, I simply copy and integrate this foundational script into my log file. This practice ensures a consistent and reproducible structure for my logs, streamlining the documentation process and maintaining clarity throughout various analyses.\n\n2.  TNT - The Next Thing Approach:\n\nDrawing inspiration from the Getting Things Done (GTD) concept, I conclude each of my log files with a TNT section---The Next Thing. This forward-looking approach acts as a reminder for my next steps in the analysis. When I resume my work, I seamlessly pick up where I left off, enhancing efficiency and maintaining a smooth workflow. By incorporating these two practices, I not only establish a standardized logging procedure but also adopt a proactive mindset for subsequent analytical sessions.\n\n## Takeaway\n\nThe concept of research logs, as elucidated by Scott Long in his book on the workflow of data analysis using Stata, made a lasting impression on my approach to work. Long emphasizes the pivotal role of logging in keeping the work on track, defining log files as a comprehensive \"record of what you have done, why you did it, and how you did that.\" [@long2008workflowdataanalysis] In aligning with this philosophy, the \"logr\" package has proven instrumental in seamlessly implementing and maintaining this disciplined logging process. It has not only enabled me to adhere to Long's principles but has significantly enhanced the efficiency and reproducibility of my data analysis endeavors.\n\nRead the full documentation of logr package [here](https://cran.r-project.org/web/packages/logr/logr.pdf).\n",
    "supporting": [
      "logr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}